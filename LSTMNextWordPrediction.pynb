{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrqnWnxixwKgTX8pKlmkBL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VibhaeoM01/ML-/blob/main/LSTMNextWordPrediction.pynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sk3kDjwHPykp"
      },
      "outputs": [],
      "source": [
        "jokes=\"\"\"Why don't eggs tell jokes? They'd crack each other up.\n",
        "What do you call a bear with no teeth? A gummy bear.\n",
        "Why was the math book sad? It had too many problems.\n",
        "Why did the scarecrow become a successful neurosurgeon? He was outstanding in his field!\n",
        "How does Moses make his coffee? Hebrews it.\n",
        "Why was the computer cold? It left its Windows open.\n",
        "Why was Cinderella so bad at soccer? She kept running away from the ball.\n",
        "What’s a skeleton’s least favorite room in the house? The living room.\n",
        "Why don't some couples go to the gym? Because some relationships don't work out.\n",
        "Why did the chicken go to the séance? To talk to the other side.\n",
        "What do you call a factory that makes okay products? A satisfactory.\n",
        "How many tickles does it take to make an octopus laugh? Ten-tickles.\n",
        "Why did the bicycle stand on its own? It was two-tired.\n",
        "Why don't crabs give to charity? Because they’re shellfish.\n",
        "What do you call a fish with no eyes? A fsh.\n",
        "What do you get when you cross an elephant with a rhinoceros? Elephino!\n",
        "Why did the golfer bring an extra pair of pants? In case he got a hole in one.\n",
        "How do you find Will Smith in the snow? Look for fresh prints.\n",
        "Why did the scarecrow become a motivational speaker? He was outstanding in his field.\n",
        "Why don’t skeletons fight each other? They don’t have the guts.\n",
        "What’s brown and sticky? A stick.\n",
        "Why did the orange stop? It ran out of juice.\n",
        "How do you make a tissue dance? You put a little boogie in it.\n",
        "What do you call a magic dog? A labracadabrador.\n",
        "Why don’t scientists trust atoms? Because they make up everything!\n",
        "Why did the music teacher need a ladder? To reach the high notes.\n",
        "What do you get when you cross a snowman with a dog? Frostbite.\n",
        "What’s the best way to watch a fly fishing tournament? Live stream.\n",
        "Why don’t some fish play piano? Because you can’t tuna fish.\n",
        "How does a vampire start a letter? Tomb it may concern.\n",
        "Why don't sharks eat clowns? They taste funny.\n",
        "Why did the cookie go to the hospital? Because it felt crummy.\n",
        "What did the janitor say when he jumped out of the closet? Supplies!\n",
        "Why don’t ants get sick? They have tiny ant-bodies.\n",
        "How do you make holy water? You boil the hell out of it.\n",
        "What do you call an elephant that doesn’t matter? An irrelephant.\n",
        "Why did the scarecrow become a neurosurgeon? He was outstanding in his field.\n",
        "Why did the banana go to the doctor? It wasn’t peeling well.\n",
        "Why did the golfer bring two pairs of pants? In case he got a hole in one.\n",
        "How do cows stay up to date? They read the moos-paper.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZuWW288RwnK",
        "outputId": "d50e469d-9079-49ee-e590-3af74d2db2eb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()"
      ],
      "metadata": {
        "id": "1tpB2kpaR3lX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([jokes])"
      ],
      "metadata": {
        "id": "8Lo1npBvSD_F"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYtXP20XSRq7",
        "outputId": "7baea791-1016-46ec-e2ff-dfb4a4ef4246"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'a': 2,\n",
              " 'why': 3,\n",
              " 'you': 4,\n",
              " 'did': 5,\n",
              " 'do': 6,\n",
              " 'it': 7,\n",
              " 'to': 8,\n",
              " 'in': 9,\n",
              " 'what': 10,\n",
              " 'was': 11,\n",
              " 'how': 12,\n",
              " 'he': 13,\n",
              " \"don't\": 14,\n",
              " 'call': 15,\n",
              " 'make': 16,\n",
              " 'because': 17,\n",
              " 'an': 18,\n",
              " 'of': 19,\n",
              " 'don’t': 20,\n",
              " 'they': 21,\n",
              " 'with': 22,\n",
              " 'his': 23,\n",
              " 'go': 24,\n",
              " 'out': 25,\n",
              " 'other': 26,\n",
              " 'up': 27,\n",
              " 'scarecrow': 28,\n",
              " 'become': 29,\n",
              " 'outstanding': 30,\n",
              " 'field': 31,\n",
              " 'does': 32,\n",
              " 'what’s': 33,\n",
              " 'some': 34,\n",
              " 'fish': 35,\n",
              " 'get': 36,\n",
              " 'when': 37,\n",
              " 'each': 38,\n",
              " 'bear': 39,\n",
              " 'no': 40,\n",
              " 'many': 41,\n",
              " 'neurosurgeon': 42,\n",
              " 'its': 43,\n",
              " 'room': 44,\n",
              " 'that': 45,\n",
              " 'tickles': 46,\n",
              " 'two': 47,\n",
              " 'cross': 48,\n",
              " 'elephant': 49,\n",
              " 'golfer': 50,\n",
              " 'bring': 51,\n",
              " 'pants': 52,\n",
              " 'case': 53,\n",
              " 'got': 54,\n",
              " 'hole': 55,\n",
              " 'one': 56,\n",
              " 'have': 57,\n",
              " 'dog': 58,\n",
              " 'eggs': 59,\n",
              " 'tell': 60,\n",
              " 'jokes': 61,\n",
              " \"they'd\": 62,\n",
              " 'crack': 63,\n",
              " 'teeth': 64,\n",
              " 'gummy': 65,\n",
              " 'math': 66,\n",
              " 'book': 67,\n",
              " 'sad': 68,\n",
              " 'had': 69,\n",
              " 'too': 70,\n",
              " 'problems': 71,\n",
              " 'successful': 72,\n",
              " 'moses': 73,\n",
              " 'coffee': 74,\n",
              " 'hebrews': 75,\n",
              " 'computer': 76,\n",
              " 'cold': 77,\n",
              " 'left': 78,\n",
              " 'windows': 79,\n",
              " 'open': 80,\n",
              " 'cinderella': 81,\n",
              " 'so': 82,\n",
              " 'bad': 83,\n",
              " 'at': 84,\n",
              " 'soccer': 85,\n",
              " 'she': 86,\n",
              " 'kept': 87,\n",
              " 'running': 88,\n",
              " 'away': 89,\n",
              " 'from': 90,\n",
              " 'ball': 91,\n",
              " 'skeleton’s': 92,\n",
              " 'least': 93,\n",
              " 'favorite': 94,\n",
              " 'house': 95,\n",
              " 'living': 96,\n",
              " 'couples': 97,\n",
              " 'gym': 98,\n",
              " 'relationships': 99,\n",
              " 'work': 100,\n",
              " 'chicken': 101,\n",
              " 'séance': 102,\n",
              " 'talk': 103,\n",
              " 'side': 104,\n",
              " 'factory': 105,\n",
              " 'makes': 106,\n",
              " 'okay': 107,\n",
              " 'products': 108,\n",
              " 'satisfactory': 109,\n",
              " 'take': 110,\n",
              " 'octopus': 111,\n",
              " 'laugh': 112,\n",
              " 'ten': 113,\n",
              " 'bicycle': 114,\n",
              " 'stand': 115,\n",
              " 'on': 116,\n",
              " 'own': 117,\n",
              " 'tired': 118,\n",
              " 'crabs': 119,\n",
              " 'give': 120,\n",
              " 'charity': 121,\n",
              " 'they’re': 122,\n",
              " 'shellfish': 123,\n",
              " 'eyes': 124,\n",
              " 'fsh': 125,\n",
              " 'rhinoceros': 126,\n",
              " 'elephino': 127,\n",
              " 'extra': 128,\n",
              " 'pair': 129,\n",
              " 'find': 130,\n",
              " 'will': 131,\n",
              " 'smith': 132,\n",
              " 'snow': 133,\n",
              " 'look': 134,\n",
              " 'for': 135,\n",
              " 'fresh': 136,\n",
              " 'prints': 137,\n",
              " 'motivational': 138,\n",
              " 'speaker': 139,\n",
              " 'skeletons': 140,\n",
              " 'fight': 141,\n",
              " 'guts': 142,\n",
              " 'brown': 143,\n",
              " 'and': 144,\n",
              " 'sticky': 145,\n",
              " 'stick': 146,\n",
              " 'orange': 147,\n",
              " 'stop': 148,\n",
              " 'ran': 149,\n",
              " 'juice': 150,\n",
              " 'tissue': 151,\n",
              " 'dance': 152,\n",
              " 'put': 153,\n",
              " 'little': 154,\n",
              " 'boogie': 155,\n",
              " 'magic': 156,\n",
              " 'labracadabrador': 157,\n",
              " 'scientists': 158,\n",
              " 'trust': 159,\n",
              " 'atoms': 160,\n",
              " 'everything': 161,\n",
              " 'music': 162,\n",
              " 'teacher': 163,\n",
              " 'need': 164,\n",
              " 'ladder': 165,\n",
              " 'reach': 166,\n",
              " 'high': 167,\n",
              " 'notes': 168,\n",
              " 'snowman': 169,\n",
              " 'frostbite': 170,\n",
              " 'best': 171,\n",
              " 'way': 172,\n",
              " 'watch': 173,\n",
              " 'fly': 174,\n",
              " 'fishing': 175,\n",
              " 'tournament': 176,\n",
              " 'live': 177,\n",
              " 'stream': 178,\n",
              " 'play': 179,\n",
              " 'piano': 180,\n",
              " 'can’t': 181,\n",
              " 'tuna': 182,\n",
              " 'vampire': 183,\n",
              " 'start': 184,\n",
              " 'letter': 185,\n",
              " 'tomb': 186,\n",
              " 'may': 187,\n",
              " 'concern': 188,\n",
              " 'sharks': 189,\n",
              " 'eat': 190,\n",
              " 'clowns': 191,\n",
              " 'taste': 192,\n",
              " 'funny': 193,\n",
              " 'cookie': 194,\n",
              " 'hospital': 195,\n",
              " 'felt': 196,\n",
              " 'crummy': 197,\n",
              " 'janitor': 198,\n",
              " 'say': 199,\n",
              " 'jumped': 200,\n",
              " 'closet': 201,\n",
              " 'supplies': 202,\n",
              " 'ants': 203,\n",
              " 'sick': 204,\n",
              " 'tiny': 205,\n",
              " 'ant': 206,\n",
              " 'bodies': 207,\n",
              " 'holy': 208,\n",
              " 'water': 209,\n",
              " 'boil': 210,\n",
              " 'hell': 211,\n",
              " 'doesn’t': 212,\n",
              " 'matter': 213,\n",
              " 'irrelephant': 214,\n",
              " 'banana': 215,\n",
              " 'doctor': 216,\n",
              " 'wasn’t': 217,\n",
              " 'peeling': 218,\n",
              " 'well': 219,\n",
              " 'pairs': 220,\n",
              " 'cows': 221,\n",
              " 'stay': 222,\n",
              " 'date': 223,\n",
              " 'read': 224,\n",
              " 'moos': 225,\n",
              " 'paper': 226}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZhTwn7KZt3c",
        "outputId": "c9ae1ead-66c4-46b5-8e19-d00b1111f197"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "226"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences=[]\n",
        "for sentence in jokes.split('\\n'):\n",
        "  tokenized_sentence=tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "  for i in range(1,len(tokenized_sentence)):\n",
        "    input_sequences.append(tokenized_sentence[:i+1])"
      ],
      "metadata": {
        "id": "wkGGRzsSSeRV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHHL0ErUSse7",
        "outputId": "403a6664-345e-455d-f530-0936b1d1a01e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 14],\n",
              " [3, 14, 59],\n",
              " [3, 14, 59, 60],\n",
              " [3, 14, 59, 60, 61],\n",
              " [3, 14, 59, 60, 61, 62],\n",
              " [3, 14, 59, 60, 61, 62, 63],\n",
              " [3, 14, 59, 60, 61, 62, 63, 38],\n",
              " [3, 14, 59, 60, 61, 62, 63, 38, 26],\n",
              " [3, 14, 59, 60, 61, 62, 63, 38, 26, 27],\n",
              " [10, 6],\n",
              " [10, 6, 4],\n",
              " [10, 6, 4, 15],\n",
              " [10, 6, 4, 15, 2],\n",
              " [10, 6, 4, 15, 2, 39],\n",
              " [10, 6, 4, 15, 2, 39, 22],\n",
              " [10, 6, 4, 15, 2, 39, 22, 40],\n",
              " [10, 6, 4, 15, 2, 39, 22, 40, 64],\n",
              " [10, 6, 4, 15, 2, 39, 22, 40, 64, 2],\n",
              " [10, 6, 4, 15, 2, 39, 22, 40, 64, 2, 65],\n",
              " [10, 6, 4, 15, 2, 39, 22, 40, 64, 2, 65, 39],\n",
              " [3, 11],\n",
              " [3, 11, 1],\n",
              " [3, 11, 1, 66],\n",
              " [3, 11, 1, 66, 67],\n",
              " [3, 11, 1, 66, 67, 68],\n",
              " [3, 11, 1, 66, 67, 68, 7],\n",
              " [3, 11, 1, 66, 67, 68, 7, 69],\n",
              " [3, 11, 1, 66, 67, 68, 7, 69, 70],\n",
              " [3, 11, 1, 66, 67, 68, 7, 69, 70, 41],\n",
              " [3, 11, 1, 66, 67, 68, 7, 69, 70, 41, 71],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 28],\n",
              " [3, 5, 1, 28, 29],\n",
              " [3, 5, 1, 28, 29, 2],\n",
              " [3, 5, 1, 28, 29, 2, 72],\n",
              " [3, 5, 1, 28, 29, 2, 72, 42],\n",
              " [3, 5, 1, 28, 29, 2, 72, 42, 13],\n",
              " [3, 5, 1, 28, 29, 2, 72, 42, 13, 11],\n",
              " [3, 5, 1, 28, 29, 2, 72, 42, 13, 11, 30],\n",
              " [3, 5, 1, 28, 29, 2, 72, 42, 13, 11, 30, 9],\n",
              " [3, 5, 1, 28, 29, 2, 72, 42, 13, 11, 30, 9, 23],\n",
              " [3, 5, 1, 28, 29, 2, 72, 42, 13, 11, 30, 9, 23, 31],\n",
              " [12, 32],\n",
              " [12, 32, 73],\n",
              " [12, 32, 73, 16],\n",
              " [12, 32, 73, 16, 23],\n",
              " [12, 32, 73, 16, 23, 74],\n",
              " [12, 32, 73, 16, 23, 74, 75],\n",
              " [12, 32, 73, 16, 23, 74, 75, 7],\n",
              " [3, 11],\n",
              " [3, 11, 1],\n",
              " [3, 11, 1, 76],\n",
              " [3, 11, 1, 76, 77],\n",
              " [3, 11, 1, 76, 77, 7],\n",
              " [3, 11, 1, 76, 77, 7, 78],\n",
              " [3, 11, 1, 76, 77, 7, 78, 43],\n",
              " [3, 11, 1, 76, 77, 7, 78, 43, 79],\n",
              " [3, 11, 1, 76, 77, 7, 78, 43, 79, 80],\n",
              " [3, 11],\n",
              " [3, 11, 81],\n",
              " [3, 11, 81, 82],\n",
              " [3, 11, 81, 82, 83],\n",
              " [3, 11, 81, 82, 83, 84],\n",
              " [3, 11, 81, 82, 83, 84, 85],\n",
              " [3, 11, 81, 82, 83, 84, 85, 86],\n",
              " [3, 11, 81, 82, 83, 84, 85, 86, 87],\n",
              " [3, 11, 81, 82, 83, 84, 85, 86, 87, 88],\n",
              " [3, 11, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
              " [3, 11, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90],\n",
              " [3, 11, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 1],\n",
              " [3, 11, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 1, 91],\n",
              " [33, 2],\n",
              " [33, 2, 92],\n",
              " [33, 2, 92, 93],\n",
              " [33, 2, 92, 93, 94],\n",
              " [33, 2, 92, 93, 94, 44],\n",
              " [33, 2, 92, 93, 94, 44, 9],\n",
              " [33, 2, 92, 93, 94, 44, 9, 1],\n",
              " [33, 2, 92, 93, 94, 44, 9, 1, 95],\n",
              " [33, 2, 92, 93, 94, 44, 9, 1, 95, 1],\n",
              " [33, 2, 92, 93, 94, 44, 9, 1, 95, 1, 96],\n",
              " [33, 2, 92, 93, 94, 44, 9, 1, 95, 1, 96, 44],\n",
              " [3, 14],\n",
              " [3, 14, 34],\n",
              " [3, 14, 34, 97],\n",
              " [3, 14, 34, 97, 24],\n",
              " [3, 14, 34, 97, 24, 8],\n",
              " [3, 14, 34, 97, 24, 8, 1],\n",
              " [3, 14, 34, 97, 24, 8, 1, 98],\n",
              " [3, 14, 34, 97, 24, 8, 1, 98, 17],\n",
              " [3, 14, 34, 97, 24, 8, 1, 98, 17, 34],\n",
              " [3, 14, 34, 97, 24, 8, 1, 98, 17, 34, 99],\n",
              " [3, 14, 34, 97, 24, 8, 1, 98, 17, 34, 99, 14],\n",
              " [3, 14, 34, 97, 24, 8, 1, 98, 17, 34, 99, 14, 100],\n",
              " [3, 14, 34, 97, 24, 8, 1, 98, 17, 34, 99, 14, 100, 25],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 101],\n",
              " [3, 5, 1, 101, 24],\n",
              " [3, 5, 1, 101, 24, 8],\n",
              " [3, 5, 1, 101, 24, 8, 1],\n",
              " [3, 5, 1, 101, 24, 8, 1, 102],\n",
              " [3, 5, 1, 101, 24, 8, 1, 102, 8],\n",
              " [3, 5, 1, 101, 24, 8, 1, 102, 8, 103],\n",
              " [3, 5, 1, 101, 24, 8, 1, 102, 8, 103, 8],\n",
              " [3, 5, 1, 101, 24, 8, 1, 102, 8, 103, 8, 1],\n",
              " [3, 5, 1, 101, 24, 8, 1, 102, 8, 103, 8, 1, 26],\n",
              " [3, 5, 1, 101, 24, 8, 1, 102, 8, 103, 8, 1, 26, 104],\n",
              " [10, 6],\n",
              " [10, 6, 4],\n",
              " [10, 6, 4, 15],\n",
              " [10, 6, 4, 15, 2],\n",
              " [10, 6, 4, 15, 2, 105],\n",
              " [10, 6, 4, 15, 2, 105, 45],\n",
              " [10, 6, 4, 15, 2, 105, 45, 106],\n",
              " [10, 6, 4, 15, 2, 105, 45, 106, 107],\n",
              " [10, 6, 4, 15, 2, 105, 45, 106, 107, 108],\n",
              " [10, 6, 4, 15, 2, 105, 45, 106, 107, 108, 2],\n",
              " [10, 6, 4, 15, 2, 105, 45, 106, 107, 108, 2, 109],\n",
              " [12, 41],\n",
              " [12, 41, 46],\n",
              " [12, 41, 46, 32],\n",
              " [12, 41, 46, 32, 7],\n",
              " [12, 41, 46, 32, 7, 110],\n",
              " [12, 41, 46, 32, 7, 110, 8],\n",
              " [12, 41, 46, 32, 7, 110, 8, 16],\n",
              " [12, 41, 46, 32, 7, 110, 8, 16, 18],\n",
              " [12, 41, 46, 32, 7, 110, 8, 16, 18, 111],\n",
              " [12, 41, 46, 32, 7, 110, 8, 16, 18, 111, 112],\n",
              " [12, 41, 46, 32, 7, 110, 8, 16, 18, 111, 112, 113],\n",
              " [12, 41, 46, 32, 7, 110, 8, 16, 18, 111, 112, 113, 46],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 114],\n",
              " [3, 5, 1, 114, 115],\n",
              " [3, 5, 1, 114, 115, 116],\n",
              " [3, 5, 1, 114, 115, 116, 43],\n",
              " [3, 5, 1, 114, 115, 116, 43, 117],\n",
              " [3, 5, 1, 114, 115, 116, 43, 117, 7],\n",
              " [3, 5, 1, 114, 115, 116, 43, 117, 7, 11],\n",
              " [3, 5, 1, 114, 115, 116, 43, 117, 7, 11, 47],\n",
              " [3, 5, 1, 114, 115, 116, 43, 117, 7, 11, 47, 118],\n",
              " [3, 14],\n",
              " [3, 14, 119],\n",
              " [3, 14, 119, 120],\n",
              " [3, 14, 119, 120, 8],\n",
              " [3, 14, 119, 120, 8, 121],\n",
              " [3, 14, 119, 120, 8, 121, 17],\n",
              " [3, 14, 119, 120, 8, 121, 17, 122],\n",
              " [3, 14, 119, 120, 8, 121, 17, 122, 123],\n",
              " [10, 6],\n",
              " [10, 6, 4],\n",
              " [10, 6, 4, 15],\n",
              " [10, 6, 4, 15, 2],\n",
              " [10, 6, 4, 15, 2, 35],\n",
              " [10, 6, 4, 15, 2, 35, 22],\n",
              " [10, 6, 4, 15, 2, 35, 22, 40],\n",
              " [10, 6, 4, 15, 2, 35, 22, 40, 124],\n",
              " [10, 6, 4, 15, 2, 35, 22, 40, 124, 2],\n",
              " [10, 6, 4, 15, 2, 35, 22, 40, 124, 2, 125],\n",
              " [10, 6],\n",
              " [10, 6, 4],\n",
              " [10, 6, 4, 36],\n",
              " [10, 6, 4, 36, 37],\n",
              " [10, 6, 4, 36, 37, 4],\n",
              " [10, 6, 4, 36, 37, 4, 48],\n",
              " [10, 6, 4, 36, 37, 4, 48, 18],\n",
              " [10, 6, 4, 36, 37, 4, 48, 18, 49],\n",
              " [10, 6, 4, 36, 37, 4, 48, 18, 49, 22],\n",
              " [10, 6, 4, 36, 37, 4, 48, 18, 49, 22, 2],\n",
              " [10, 6, 4, 36, 37, 4, 48, 18, 49, 22, 2, 126],\n",
              " [10, 6, 4, 36, 37, 4, 48, 18, 49, 22, 2, 126, 127],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 50],\n",
              " [3, 5, 1, 50, 51],\n",
              " [3, 5, 1, 50, 51, 18],\n",
              " [3, 5, 1, 50, 51, 18, 128],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19, 52],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19, 52, 9],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19, 52, 9, 53],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19, 52, 9, 53, 13],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19, 52, 9, 53, 13, 54],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19, 52, 9, 53, 13, 54, 2],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19, 52, 9, 53, 13, 54, 2, 55],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19, 52, 9, 53, 13, 54, 2, 55, 9],\n",
              " [3, 5, 1, 50, 51, 18, 128, 129, 19, 52, 9, 53, 13, 54, 2, 55, 9, 56],\n",
              " [12, 6],\n",
              " [12, 6, 4],\n",
              " [12, 6, 4, 130],\n",
              " [12, 6, 4, 130, 131],\n",
              " [12, 6, 4, 130, 131, 132],\n",
              " [12, 6, 4, 130, 131, 132, 9],\n",
              " [12, 6, 4, 130, 131, 132, 9, 1],\n",
              " [12, 6, 4, 130, 131, 132, 9, 1, 133],\n",
              " [12, 6, 4, 130, 131, 132, 9, 1, 133, 134],\n",
              " [12, 6, 4, 130, 131, 132, 9, 1, 133, 134, 135],\n",
              " [12, 6, 4, 130, 131, 132, 9, 1, 133, 134, 135, 136],\n",
              " [12, 6, 4, 130, 131, 132, 9, 1, 133, 134, 135, 136, 137],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 28],\n",
              " [3, 5, 1, 28, 29],\n",
              " [3, 5, 1, 28, 29, 2],\n",
              " [3, 5, 1, 28, 29, 2, 138],\n",
              " [3, 5, 1, 28, 29, 2, 138, 139],\n",
              " [3, 5, 1, 28, 29, 2, 138, 139, 13],\n",
              " [3, 5, 1, 28, 29, 2, 138, 139, 13, 11],\n",
              " [3, 5, 1, 28, 29, 2, 138, 139, 13, 11, 30],\n",
              " [3, 5, 1, 28, 29, 2, 138, 139, 13, 11, 30, 9],\n",
              " [3, 5, 1, 28, 29, 2, 138, 139, 13, 11, 30, 9, 23],\n",
              " [3, 5, 1, 28, 29, 2, 138, 139, 13, 11, 30, 9, 23, 31],\n",
              " [3, 20],\n",
              " [3, 20, 140],\n",
              " [3, 20, 140, 141],\n",
              " [3, 20, 140, 141, 38],\n",
              " [3, 20, 140, 141, 38, 26],\n",
              " [3, 20, 140, 141, 38, 26, 21],\n",
              " [3, 20, 140, 141, 38, 26, 21, 20],\n",
              " [3, 20, 140, 141, 38, 26, 21, 20, 57],\n",
              " [3, 20, 140, 141, 38, 26, 21, 20, 57, 1],\n",
              " [3, 20, 140, 141, 38, 26, 21, 20, 57, 1, 142],\n",
              " [33, 143],\n",
              " [33, 143, 144],\n",
              " [33, 143, 144, 145],\n",
              " [33, 143, 144, 145, 2],\n",
              " [33, 143, 144, 145, 2, 146],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 147],\n",
              " [3, 5, 1, 147, 148],\n",
              " [3, 5, 1, 147, 148, 7],\n",
              " [3, 5, 1, 147, 148, 7, 149],\n",
              " [3, 5, 1, 147, 148, 7, 149, 25],\n",
              " [3, 5, 1, 147, 148, 7, 149, 25, 19],\n",
              " [3, 5, 1, 147, 148, 7, 149, 25, 19, 150],\n",
              " [12, 6],\n",
              " [12, 6, 4],\n",
              " [12, 6, 4, 16],\n",
              " [12, 6, 4, 16, 2],\n",
              " [12, 6, 4, 16, 2, 151],\n",
              " [12, 6, 4, 16, 2, 151, 152],\n",
              " [12, 6, 4, 16, 2, 151, 152, 4],\n",
              " [12, 6, 4, 16, 2, 151, 152, 4, 153],\n",
              " [12, 6, 4, 16, 2, 151, 152, 4, 153, 2],\n",
              " [12, 6, 4, 16, 2, 151, 152, 4, 153, 2, 154],\n",
              " [12, 6, 4, 16, 2, 151, 152, 4, 153, 2, 154, 155],\n",
              " [12, 6, 4, 16, 2, 151, 152, 4, 153, 2, 154, 155, 9],\n",
              " [12, 6, 4, 16, 2, 151, 152, 4, 153, 2, 154, 155, 9, 7],\n",
              " [10, 6],\n",
              " [10, 6, 4],\n",
              " [10, 6, 4, 15],\n",
              " [10, 6, 4, 15, 2],\n",
              " [10, 6, 4, 15, 2, 156],\n",
              " [10, 6, 4, 15, 2, 156, 58],\n",
              " [10, 6, 4, 15, 2, 156, 58, 2],\n",
              " [10, 6, 4, 15, 2, 156, 58, 2, 157],\n",
              " [3, 20],\n",
              " [3, 20, 158],\n",
              " [3, 20, 158, 159],\n",
              " [3, 20, 158, 159, 160],\n",
              " [3, 20, 158, 159, 160, 17],\n",
              " [3, 20, 158, 159, 160, 17, 21],\n",
              " [3, 20, 158, 159, 160, 17, 21, 16],\n",
              " [3, 20, 158, 159, 160, 17, 21, 16, 27],\n",
              " [3, 20, 158, 159, 160, 17, 21, 16, 27, 161],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 162],\n",
              " [3, 5, 1, 162, 163],\n",
              " [3, 5, 1, 162, 163, 164],\n",
              " [3, 5, 1, 162, 163, 164, 2],\n",
              " [3, 5, 1, 162, 163, 164, 2, 165],\n",
              " [3, 5, 1, 162, 163, 164, 2, 165, 8],\n",
              " [3, 5, 1, 162, 163, 164, 2, 165, 8, 166],\n",
              " [3, 5, 1, 162, 163, 164, 2, 165, 8, 166, 1],\n",
              " [3, 5, 1, 162, 163, 164, 2, 165, 8, 166, 1, 167],\n",
              " [3, 5, 1, 162, 163, 164, 2, 165, 8, 166, 1, 167, 168],\n",
              " [10, 6],\n",
              " [10, 6, 4],\n",
              " [10, 6, 4, 36],\n",
              " [10, 6, 4, 36, 37],\n",
              " [10, 6, 4, 36, 37, 4],\n",
              " [10, 6, 4, 36, 37, 4, 48],\n",
              " [10, 6, 4, 36, 37, 4, 48, 2],\n",
              " [10, 6, 4, 36, 37, 4, 48, 2, 169],\n",
              " [10, 6, 4, 36, 37, 4, 48, 2, 169, 22],\n",
              " [10, 6, 4, 36, 37, 4, 48, 2, 169, 22, 2],\n",
              " [10, 6, 4, 36, 37, 4, 48, 2, 169, 22, 2, 58],\n",
              " [10, 6, 4, 36, 37, 4, 48, 2, 169, 22, 2, 58, 170],\n",
              " [33, 1],\n",
              " [33, 1, 171],\n",
              " [33, 1, 171, 172],\n",
              " [33, 1, 171, 172, 8],\n",
              " [33, 1, 171, 172, 8, 173],\n",
              " [33, 1, 171, 172, 8, 173, 2],\n",
              " [33, 1, 171, 172, 8, 173, 2, 174],\n",
              " [33, 1, 171, 172, 8, 173, 2, 174, 175],\n",
              " [33, 1, 171, 172, 8, 173, 2, 174, 175, 176],\n",
              " [33, 1, 171, 172, 8, 173, 2, 174, 175, 176, 177],\n",
              " [33, 1, 171, 172, 8, 173, 2, 174, 175, 176, 177, 178],\n",
              " [3, 20],\n",
              " [3, 20, 34],\n",
              " [3, 20, 34, 35],\n",
              " [3, 20, 34, 35, 179],\n",
              " [3, 20, 34, 35, 179, 180],\n",
              " [3, 20, 34, 35, 179, 180, 17],\n",
              " [3, 20, 34, 35, 179, 180, 17, 4],\n",
              " [3, 20, 34, 35, 179, 180, 17, 4, 181],\n",
              " [3, 20, 34, 35, 179, 180, 17, 4, 181, 182],\n",
              " [3, 20, 34, 35, 179, 180, 17, 4, 181, 182, 35],\n",
              " [12, 32],\n",
              " [12, 32, 2],\n",
              " [12, 32, 2, 183],\n",
              " [12, 32, 2, 183, 184],\n",
              " [12, 32, 2, 183, 184, 2],\n",
              " [12, 32, 2, 183, 184, 2, 185],\n",
              " [12, 32, 2, 183, 184, 2, 185, 186],\n",
              " [12, 32, 2, 183, 184, 2, 185, 186, 7],\n",
              " [12, 32, 2, 183, 184, 2, 185, 186, 7, 187],\n",
              " [12, 32, 2, 183, 184, 2, 185, 186, 7, 187, 188],\n",
              " [3, 14],\n",
              " [3, 14, 189],\n",
              " [3, 14, 189, 190],\n",
              " [3, 14, 189, 190, 191],\n",
              " [3, 14, 189, 190, 191, 21],\n",
              " [3, 14, 189, 190, 191, 21, 192],\n",
              " [3, 14, 189, 190, 191, 21, 192, 193],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 194],\n",
              " [3, 5, 1, 194, 24],\n",
              " [3, 5, 1, 194, 24, 8],\n",
              " [3, 5, 1, 194, 24, 8, 1],\n",
              " [3, 5, 1, 194, 24, 8, 1, 195],\n",
              " [3, 5, 1, 194, 24, 8, 1, 195, 17],\n",
              " [3, 5, 1, 194, 24, 8, 1, 195, 17, 7],\n",
              " [3, 5, 1, 194, 24, 8, 1, 195, 17, 7, 196],\n",
              " [3, 5, 1, 194, 24, 8, 1, 195, 17, 7, 196, 197],\n",
              " [10, 5],\n",
              " [10, 5, 1],\n",
              " [10, 5, 1, 198],\n",
              " [10, 5, 1, 198, 199],\n",
              " [10, 5, 1, 198, 199, 37],\n",
              " [10, 5, 1, 198, 199, 37, 13],\n",
              " [10, 5, 1, 198, 199, 37, 13, 200],\n",
              " [10, 5, 1, 198, 199, 37, 13, 200, 25],\n",
              " [10, 5, 1, 198, 199, 37, 13, 200, 25, 19],\n",
              " [10, 5, 1, 198, 199, 37, 13, 200, 25, 19, 1],\n",
              " [10, 5, 1, 198, 199, 37, 13, 200, 25, 19, 1, 201],\n",
              " [10, 5, 1, 198, 199, 37, 13, 200, 25, 19, 1, 201, 202],\n",
              " [3, 20],\n",
              " [3, 20, 203],\n",
              " [3, 20, 203, 36],\n",
              " [3, 20, 203, 36, 204],\n",
              " [3, 20, 203, 36, 204, 21],\n",
              " [3, 20, 203, 36, 204, 21, 57],\n",
              " [3, 20, 203, 36, 204, 21, 57, 205],\n",
              " [3, 20, 203, 36, 204, 21, 57, 205, 206],\n",
              " [3, 20, 203, 36, 204, 21, 57, 205, 206, 207],\n",
              " [12, 6],\n",
              " [12, 6, 4],\n",
              " [12, 6, 4, 16],\n",
              " [12, 6, 4, 16, 208],\n",
              " [12, 6, 4, 16, 208, 209],\n",
              " [12, 6, 4, 16, 208, 209, 4],\n",
              " [12, 6, 4, 16, 208, 209, 4, 210],\n",
              " [12, 6, 4, 16, 208, 209, 4, 210, 1],\n",
              " [12, 6, 4, 16, 208, 209, 4, 210, 1, 211],\n",
              " [12, 6, 4, 16, 208, 209, 4, 210, 1, 211, 25],\n",
              " [12, 6, 4, 16, 208, 209, 4, 210, 1, 211, 25, 19],\n",
              " [12, 6, 4, 16, 208, 209, 4, 210, 1, 211, 25, 19, 7],\n",
              " [10, 6],\n",
              " [10, 6, 4],\n",
              " [10, 6, 4, 15],\n",
              " [10, 6, 4, 15, 18],\n",
              " [10, 6, 4, 15, 18, 49],\n",
              " [10, 6, 4, 15, 18, 49, 45],\n",
              " [10, 6, 4, 15, 18, 49, 45, 212],\n",
              " [10, 6, 4, 15, 18, 49, 45, 212, 213],\n",
              " [10, 6, 4, 15, 18, 49, 45, 212, 213, 18],\n",
              " [10, 6, 4, 15, 18, 49, 45, 212, 213, 18, 214],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 28],\n",
              " [3, 5, 1, 28, 29],\n",
              " [3, 5, 1, 28, 29, 2],\n",
              " [3, 5, 1, 28, 29, 2, 42],\n",
              " [3, 5, 1, 28, 29, 2, 42, 13],\n",
              " [3, 5, 1, 28, 29, 2, 42, 13, 11],\n",
              " [3, 5, 1, 28, 29, 2, 42, 13, 11, 30],\n",
              " [3, 5, 1, 28, 29, 2, 42, 13, 11, 30, 9],\n",
              " [3, 5, 1, 28, 29, 2, 42, 13, 11, 30, 9, 23],\n",
              " [3, 5, 1, 28, 29, 2, 42, 13, 11, 30, 9, 23, 31],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 215],\n",
              " [3, 5, 1, 215, 24],\n",
              " [3, 5, 1, 215, 24, 8],\n",
              " [3, 5, 1, 215, 24, 8, 1],\n",
              " [3, 5, 1, 215, 24, 8, 1, 216],\n",
              " [3, 5, 1, 215, 24, 8, 1, 216, 7],\n",
              " [3, 5, 1, 215, 24, 8, 1, 216, 7, 217],\n",
              " [3, 5, 1, 215, 24, 8, 1, 216, 7, 217, 218],\n",
              " [3, 5, 1, 215, 24, 8, 1, 216, 7, 217, 218, 219],\n",
              " [3, 5],\n",
              " [3, 5, 1],\n",
              " [3, 5, 1, 50],\n",
              " [3, 5, 1, 50, 51],\n",
              " [3, 5, 1, 50, 51, 47],\n",
              " [3, 5, 1, 50, 51, 47, 220],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19, 52],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19, 52, 9],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19, 52, 9, 53],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19, 52, 9, 53, 13],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19, 52, 9, 53, 13, 54],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19, 52, 9, 53, 13, 54, 2],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19, 52, 9, 53, 13, 54, 2, 55],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19, 52, 9, 53, 13, 54, 2, 55, 9],\n",
              " [3, 5, 1, 50, 51, 47, 220, 19, 52, 9, 53, 13, 54, 2, 55, 9, 56],\n",
              " [12, 6],\n",
              " [12, 6, 221],\n",
              " [12, 6, 221, 222],\n",
              " [12, 6, 221, 222, 27],\n",
              " [12, 6, 221, 222, 27, 8],\n",
              " [12, 6, 221, 222, 27, 8, 223],\n",
              " [12, 6, 221, 222, 27, 8, 223, 21],\n",
              " [12, 6, 221, 222, 27, 8, 223, 21, 224],\n",
              " [12, 6, 221, 222, 27, 8, 223, 21, 224, 1],\n",
              " [12, 6, 221, 222, 27, 8, 223, 21, 224, 1, 225],\n",
              " [12, 6, 221, 222, 27, 8, 223, 21, 224, 1, 225, 226]]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxi=max([len(x) for x in input_sequences])\n",
        "maxi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGLnkWLAU5-4",
        "outputId": "b14cb2e0-0fd9-4602-81c2-e178f9a0414e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded_input_seq=pad_sequences(input_sequences,maxlen=maxi,padding='pre')\n",
        "padded_input_seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb2Eh5roVRrc",
        "outputId": "75a00f36-3e0f-4fb4-d792-2f376163b29b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   3,  14],\n",
              "       [  0,   0,   0, ...,   3,  14,  59],\n",
              "       [  0,   0,   0, ...,  14,  59,  60],\n",
              "       ...,\n",
              "       [  0,   0,   0, ...,  21, 224,   1],\n",
              "       [  0,   0,   0, ..., 224,   1, 225],\n",
              "       [  0,   0,   0, ...,   1, 225, 226]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=padded_input_seq[:,:-1]"
      ],
      "metadata": {
        "id": "h5uCLFK1VpVS"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y=padded_input_seq[:,-1]"
      ],
      "metadata": {
        "id": "mAlcoxNQYXSY"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HiFPziJYfZh",
        "outputId": "0fa838bf-c040-4f65-c8fc-85652b262c74"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(435, 17)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcYzernLZLZ9",
        "outputId": "264845ce-dffe-4a5a-9e0c-c55661a87b77"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(435,)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y=to_categorical(y,num_classes=227)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8MRHfGNZMVg",
        "outputId": "289053b6-66e0-4b53-e1bc-82d45e9512bf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(435, 227)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aFruMUcZOHA",
        "outputId": "e650431a-8b77-4327-e184-2ae00a36857a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense"
      ],
      "metadata": {
        "id": "WurRupKaaRLA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GQFYy-V0QPW",
        "outputId": "d543704c-76f8-4c2b-c6a5-4697eab1d6e7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(227,100,input_length=17))\n",
        "model.add(LSTM(150)) #150 nodes in gates of LSTM RNN\n",
        "model.add(Dense(227,activation='softmax'))"
      ],
      "metadata": {
        "id": "0N65meMnz6cU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])    # cate.cross.entropy because we have multiclass classification\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE0r5FXd0ygA",
        "outputId": "ac373dcc-5ea4-49c3-da68-58d1282f9e37"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 17, 100)           22700     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 227)               34277     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 207577 (810.85 KB)\n",
            "Trainable params: 207577 (810.85 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYrFeKke1J8V",
        "outputId": "037209e9-6d9f-4efa-e0b1-1b59ff73e5b8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0 ...   0   0   3]\n",
            " [  0   0   0 ...   0   3  14]\n",
            " [  0   0   0 ...   3  14  59]\n",
            " ...\n",
            " [  0   0   0 ... 223  21 224]\n",
            " [  0   0   0 ...  21 224   1]\n",
            " [  0   0   0 ... 224   1 225]]\n",
            "(435, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tain\n",
        "model.fit(X,y,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZztWC_8e1Xck",
        "outputId": "2e82fad5-ef88-4e7f-e6cf-c67020e41794"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "14/14 [==============================] - 4s 34ms/step - loss: 5.3985 - accuracy: 0.0299\n",
            "Epoch 2/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 5.0997 - accuracy: 0.0667\n",
            "Epoch 3/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 4.9556 - accuracy: 0.0598\n",
            "Epoch 4/100\n",
            "14/14 [==============================] - 0s 36ms/step - loss: 4.9008 - accuracy: 0.0552\n",
            "Epoch 5/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 4.8498 - accuracy: 0.0713\n",
            "Epoch 6/100\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 4.7634 - accuracy: 0.0690\n",
            "Epoch 7/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 4.6734 - accuracy: 0.0966\n",
            "Epoch 8/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 4.5756 - accuracy: 0.1425\n",
            "Epoch 9/100\n",
            "14/14 [==============================] - 0s 32ms/step - loss: 4.4584 - accuracy: 0.1609\n",
            "Epoch 10/100\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 4.3203 - accuracy: 0.1862\n",
            "Epoch 11/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 4.1698 - accuracy: 0.1816\n",
            "Epoch 12/100\n",
            "14/14 [==============================] - 1s 48ms/step - loss: 3.9924 - accuracy: 0.1931\n",
            "Epoch 13/100\n",
            "14/14 [==============================] - 1s 35ms/step - loss: 3.8273 - accuracy: 0.2023\n",
            "Epoch 14/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 3.6301 - accuracy: 0.2138\n",
            "Epoch 15/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 3.4381 - accuracy: 0.2299\n",
            "Epoch 16/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 3.2789 - accuracy: 0.2529\n",
            "Epoch 17/100\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 3.0941 - accuracy: 0.2644\n",
            "Epoch 18/100\n",
            "14/14 [==============================] - 1s 56ms/step - loss: 2.9211 - accuracy: 0.3195\n",
            "Epoch 19/100\n",
            "14/14 [==============================] - 1s 68ms/step - loss: 2.7473 - accuracy: 0.3632\n",
            "Epoch 20/100\n",
            "14/14 [==============================] - 1s 68ms/step - loss: 2.5950 - accuracy: 0.4092\n",
            "Epoch 21/100\n",
            "14/14 [==============================] - 1s 74ms/step - loss: 2.4474 - accuracy: 0.4437\n",
            "Epoch 22/100\n",
            "14/14 [==============================] - 1s 44ms/step - loss: 2.3081 - accuracy: 0.4851\n",
            "Epoch 23/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 2.1810 - accuracy: 0.5103\n",
            "Epoch 24/100\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 2.0524 - accuracy: 0.5540\n",
            "Epoch 25/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 1.9420 - accuracy: 0.5770\n",
            "Epoch 26/100\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 1.8275 - accuracy: 0.6115\n",
            "Epoch 27/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 1.7228 - accuracy: 0.6575\n",
            "Epoch 28/100\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 1.6234 - accuracy: 0.6828\n",
            "Epoch 29/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 1.5465 - accuracy: 0.7057\n",
            "Epoch 30/100\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 1.4463 - accuracy: 0.7241\n",
            "Epoch 31/100\n",
            "14/14 [==============================] - 1s 35ms/step - loss: 1.3714 - accuracy: 0.7678\n",
            "Epoch 32/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 1.3015 - accuracy: 0.7816\n",
            "Epoch 33/100\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 1.2188 - accuracy: 0.7908\n",
            "Epoch 34/100\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 1.1704 - accuracy: 0.8184\n",
            "Epoch 35/100\n",
            "14/14 [==============================] - 1s 35ms/step - loss: 1.1106 - accuracy: 0.8299\n",
            "Epoch 36/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 1.0537 - accuracy: 0.8368\n",
            "Epoch 37/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 1.0110 - accuracy: 0.8483\n",
            "Epoch 38/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.9560 - accuracy: 0.8529\n",
            "Epoch 39/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 0.9097 - accuracy: 0.8506\n",
            "Epoch 40/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.8683 - accuracy: 0.8575\n",
            "Epoch 41/100\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 0.8323 - accuracy: 0.8621\n",
            "Epoch 42/100\n",
            "14/14 [==============================] - 1s 62ms/step - loss: 0.7942 - accuracy: 0.8736\n",
            "Epoch 43/100\n",
            "14/14 [==============================] - 1s 65ms/step - loss: 0.7647 - accuracy: 0.8828\n",
            "Epoch 44/100\n",
            "14/14 [==============================] - 1s 66ms/step - loss: 0.7387 - accuracy: 0.8828\n",
            "Epoch 45/100\n",
            "14/14 [==============================] - 1s 70ms/step - loss: 0.7038 - accuracy: 0.8782\n",
            "Epoch 46/100\n",
            "14/14 [==============================] - 1s 69ms/step - loss: 0.6799 - accuracy: 0.8713\n",
            "Epoch 47/100\n",
            "14/14 [==============================] - 0s 36ms/step - loss: 0.6565 - accuracy: 0.8805\n",
            "Epoch 48/100\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.6366 - accuracy: 0.8759\n",
            "Epoch 49/100\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.6147 - accuracy: 0.8851\n",
            "Epoch 50/100\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 0.5939 - accuracy: 0.8805\n",
            "Epoch 51/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.5714 - accuracy: 0.8851\n",
            "Epoch 52/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.5561 - accuracy: 0.8828\n",
            "Epoch 53/100\n",
            "14/14 [==============================] - 0s 36ms/step - loss: 0.5424 - accuracy: 0.8782\n",
            "Epoch 54/100\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.5268 - accuracy: 0.8851\n",
            "Epoch 55/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.5108 - accuracy: 0.8851\n",
            "Epoch 56/100\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.5010 - accuracy: 0.8828\n",
            "Epoch 57/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.4864 - accuracy: 0.8874\n",
            "Epoch 58/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.4755 - accuracy: 0.8851\n",
            "Epoch 59/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.4679 - accuracy: 0.8874\n",
            "Epoch 60/100\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.4566 - accuracy: 0.8805\n",
            "Epoch 61/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.4429 - accuracy: 0.8874\n",
            "Epoch 62/100\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.4381 - accuracy: 0.8828\n",
            "Epoch 63/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.4272 - accuracy: 0.8851\n",
            "Epoch 64/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.4242 - accuracy: 0.8805\n",
            "Epoch 65/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.4171 - accuracy: 0.8874\n",
            "Epoch 66/100\n",
            "14/14 [==============================] - 1s 63ms/step - loss: 0.4108 - accuracy: 0.8851\n",
            "Epoch 67/100\n",
            "14/14 [==============================] - 1s 66ms/step - loss: 0.3995 - accuracy: 0.8782\n",
            "Epoch 68/100\n",
            "14/14 [==============================] - 1s 65ms/step - loss: 0.3940 - accuracy: 0.8759\n",
            "Epoch 69/100\n",
            "14/14 [==============================] - 1s 69ms/step - loss: 0.3844 - accuracy: 0.8897\n",
            "Epoch 70/100\n",
            "14/14 [==============================] - 1s 40ms/step - loss: 0.3837 - accuracy: 0.8851\n",
            "Epoch 71/100\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 0.3732 - accuracy: 0.8851\n",
            "Epoch 72/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.3734 - accuracy: 0.8828\n",
            "Epoch 73/100\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 0.3658 - accuracy: 0.8805\n",
            "Epoch 74/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.3665 - accuracy: 0.8805\n",
            "Epoch 75/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 0.3598 - accuracy: 0.8782\n",
            "Epoch 76/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.3586 - accuracy: 0.8851\n",
            "Epoch 77/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 0.3503 - accuracy: 0.8874\n",
            "Epoch 78/100\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.3481 - accuracy: 0.8805\n",
            "Epoch 79/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.3426 - accuracy: 0.8851\n",
            "Epoch 80/100\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.3417 - accuracy: 0.8828\n",
            "Epoch 81/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 0.3344 - accuracy: 0.8828\n",
            "Epoch 82/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.3347 - accuracy: 0.8759\n",
            "Epoch 83/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.3321 - accuracy: 0.8805\n",
            "Epoch 84/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.3325 - accuracy: 0.8851\n",
            "Epoch 85/100\n",
            "14/14 [==============================] - 1s 35ms/step - loss: 0.3264 - accuracy: 0.8874\n",
            "Epoch 86/100\n",
            "14/14 [==============================] - 1s 37ms/step - loss: 0.3280 - accuracy: 0.8805\n",
            "Epoch 87/100\n",
            "14/14 [==============================] - 0s 35ms/step - loss: 0.3210 - accuracy: 0.8805\n",
            "Epoch 88/100\n",
            "14/14 [==============================] - 1s 36ms/step - loss: 0.3208 - accuracy: 0.8782\n",
            "Epoch 89/100\n",
            "14/14 [==============================] - 1s 54ms/step - loss: 0.3190 - accuracy: 0.8828\n",
            "Epoch 90/100\n",
            "14/14 [==============================] - 1s 67ms/step - loss: 0.3191 - accuracy: 0.8782\n",
            "Epoch 91/100\n",
            "14/14 [==============================] - 1s 67ms/step - loss: 0.3138 - accuracy: 0.8828\n",
            "Epoch 92/100\n",
            "14/14 [==============================] - 1s 69ms/step - loss: 0.3119 - accuracy: 0.8874\n",
            "Epoch 93/100\n",
            "14/14 [==============================] - 1s 67ms/step - loss: 0.3102 - accuracy: 0.8851\n",
            "Epoch 94/100\n",
            "14/14 [==============================] - 1s 50ms/step - loss: 0.3082 - accuracy: 0.8874\n",
            "Epoch 95/100\n",
            "14/14 [==============================] - 1s 38ms/step - loss: 0.3061 - accuracy: 0.8874\n",
            "Epoch 96/100\n",
            "14/14 [==============================] - 0s 33ms/step - loss: 0.3046 - accuracy: 0.8805\n",
            "Epoch 97/100\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 0.3017 - accuracy: 0.8805\n",
            "Epoch 98/100\n",
            "14/14 [==============================] - 1s 35ms/step - loss: 0.3022 - accuracy: 0.8805\n",
            "Epoch 99/100\n",
            "14/14 [==============================] - 1s 39ms/step - loss: 0.2990 - accuracy: 0.8759\n",
            "Epoch 100/100\n",
            "14/14 [==============================] - 0s 34ms/step - loss: 0.2966 - accuracy: 0.8805\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78d1dd55d210>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import numpy as np"
      ],
      "metadata": {
        "id": "zobJ6tE-6pV9"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predicting next word"
      ],
      "metadata": {
        "id": "kjbURdqk8o8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"How\"\n",
        "\n",
        "#tokenize\n",
        "token_text=tokenizer.texts_to_sequences([text])[0]   # [0] to avoid nested list\n",
        "#padding\n",
        "padding_tt=pad_sequences([token_text],maxlen=17,padding='pre')\n",
        "print(padding_tt)\n",
        "#prediction\n",
        "print(model.predict(padding_tt))\n",
        "print(model.predict(padding_tt).shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Abo90rLQ41x4",
        "outputId": "ef496b3d-a15e-4cc7-de67-fa69c580b088"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[[8.28424902e-07 6.67285000e-04 3.43556516e-04 8.91929744e-07\n",
            "  1.62444936e-04 3.94235598e-03 5.93819141e-01 1.99240225e-04\n",
            "  4.33186724e-05 7.92448845e-05 6.02368402e-07 4.99696122e-04\n",
            "  1.71052420e-06 5.40741858e-05 4.78582369e-04 1.47051062e-03\n",
            "  5.03945397e-04 5.12549059e-06 9.78763455e-06 1.73300541e-05\n",
            "  2.72186968e-04 6.82959990e-06 1.06670886e-05 1.19515011e-04\n",
            "  7.63637290e-05 6.85492432e-06 1.16019510e-05 1.01348305e-04\n",
            "  5.44817931e-06 1.22552592e-05 5.35494746e-06 1.61580101e-05\n",
            "  2.48743773e-01 4.70075321e-07 2.13705772e-03 3.03989131e-04\n",
            "  3.81150341e-04 2.00191789e-04 1.05253283e-04 3.24013272e-05\n",
            "  1.13930646e-05 1.26726046e-01 7.99474492e-06 1.47091930e-06\n",
            "  2.78917651e-05 7.85215980e-06 4.38042078e-03 1.20600826e-05\n",
            "  2.09885529e-05 1.87508758e-05 1.01683518e-05 1.49605762e-06\n",
            "  9.95973187e-06 7.60194189e-06 2.16921399e-06 2.56712127e-07\n",
            "  4.69717952e-06 1.00731017e-06 3.19934543e-06 2.37491171e-04\n",
            "  4.69934603e-05 2.77231675e-06 1.18231092e-05 2.07542689e-06\n",
            "  4.72150168e-05 1.00059997e-05 1.64131256e-06 7.92713308e-07\n",
            "  6.56353791e-07 3.17734163e-07 3.92483526e-06 1.16042756e-05\n",
            "  1.55302280e-06 7.51330925e-04 6.70941008e-05 1.76874382e-05\n",
            "  1.29155808e-06 6.78595427e-07 1.43037937e-07 7.86029261e-07\n",
            "  4.46484864e-06 3.09691939e-04 9.28729423e-05 5.65177834e-06\n",
            "  2.49848881e-06 3.41941927e-06 9.32933108e-06 2.01884700e-06\n",
            "  1.61622484e-05 4.89296799e-05 3.52855059e-05 2.98600503e-06\n",
            "  1.75175970e-04 2.15275300e-04 2.13021249e-05 2.39248607e-06\n",
            "  1.78876718e-07 5.50445548e-05 6.37162941e-07 1.25577708e-05\n",
            "  2.54203587e-05 8.93799097e-06 2.99891894e-06 1.66734644e-05\n",
            "  1.16276042e-05 4.80713015e-06 3.96049643e-07 2.17843944e-06\n",
            "  1.24894848e-06 2.66895341e-07 1.18605012e-05 3.57756363e-07\n",
            "  1.05581307e-06 1.47730464e-06 6.17752403e-06 1.60090349e-06\n",
            "  1.49900120e-06 1.78520222e-05 1.52828288e-05 1.17188742e-04\n",
            "  2.18111381e-05 1.87088826e-05 5.87654449e-06 1.87511182e-06\n",
            "  2.55757186e-05 5.61295110e-07 5.76198502e-08 1.41977864e-07\n",
            "  3.65687995e-08 4.43234455e-07 1.60351352e-04 1.51727636e-05\n",
            "  5.65799382e-07 5.02723658e-08 4.85154175e-08 4.81876327e-07\n",
            "  1.96706527e-08 3.77949647e-07 1.80710447e-06 1.02329011e-06\n",
            "  2.52756686e-03 1.75696128e-04 2.96964600e-07 1.27111317e-03\n",
            "  1.58658600e-04 3.25858964e-05 7.93393701e-06 7.36129869e-06\n",
            "  8.06368632e-07 7.03152864e-07 1.07443866e-05 6.55577196e-06\n",
            "  3.49022139e-06 4.72706779e-06 7.35528772e-07 1.79618371e-06\n",
            "  6.14300961e-06 1.14362805e-07 2.35715578e-03 1.81394033e-04\n",
            "  2.58778873e-05 4.93447487e-06 1.08058366e-05 3.59232808e-06\n",
            "  1.11676645e-05 2.07178027e-06 7.71707619e-06 1.85662444e-07\n",
            "  4.68648113e-06 4.21379241e-07 2.03106333e-06 1.53973615e-05\n",
            "  2.77273443e-06 1.78455105e-06 6.27423091e-08 2.30296541e-06\n",
            "  8.48937489e-08 9.74173872e-07 1.98384168e-06 3.14611476e-04\n",
            "  4.96844332e-05 2.50334142e-05 1.85440203e-05 3.33457028e-05\n",
            "  4.61553600e-06 9.41047233e-07 6.58371164e-07 3.79713413e-07\n",
            "  3.30207149e-06 1.31771289e-04 2.56135518e-05 2.27613486e-06\n",
            "  1.03233924e-06 7.31304169e-07 1.12623302e-05 3.80257347e-07\n",
            "  3.99546889e-06 8.52599169e-06 4.23869678e-05 6.83483522e-05\n",
            "  9.37830464e-06 4.23700112e-06 1.37951895e-06 2.43025413e-03\n",
            "  7.75484932e-06 1.22594156e-06 1.03618461e-07 1.49975250e-07\n",
            "  9.67115629e-05 1.03121667e-04 4.58705508e-05 9.51722825e-08\n",
            "  6.75052490e-07 1.10583392e-06 9.55468735e-08 5.96741302e-06\n",
            "  5.12313250e-07 4.96902032e-07 3.34007791e-06 8.00199473e-07\n",
            "  1.01321712e-06 4.92956198e-04 3.34822485e-04 1.16039546e-05\n",
            "  1.89750460e-06 1.09884972e-07 7.89856301e-07]]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "(1, 227)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos=np.argmax(model.predict(padding_tt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NUMDL057zng",
        "outputId": "2c419af2-6b54-4377-a65a-5c4f89de6072"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word,index in tokenizer.word_index.items():\n",
        "  if index==pos:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENrggAI78KFU",
        "outputId": "4da934a2-1286-4103-cfd8-adb5d51c5963"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "do\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Predicting next 5 words"
      ],
      "metadata": {
        "id": "RMTcT6Zq8tbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"How\"\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    #tokenize\n",
        "    token_text=tokenizer.texts_to_sequences([text])[0]   # [0] to avoid nested list\n",
        "    #padding\n",
        "    padding_tt=pad_sequences([token_text],maxlen=17,padding='pre')\n",
        "    print(padding_tt)\n",
        "    #prediction\n",
        "    # print(model.predict(padding_tt))\n",
        "    # print(model.predict(padding_tt).shape)\n",
        "    pos=np.argmax(model.predict(padding_tt))\n",
        "\n",
        "    for word,index in tokenizer.word_index.items():\n",
        "      if index == pos:\n",
        "        text=text+ \" \"+ word\n",
        "        print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBILBMhH8OPk",
        "outputId": "f510e89e-64bf-4a7b-8801-e366f5c07b40"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12]]\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "How do\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  6]]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "How do you\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 12  6  4]]\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "How do you make\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0 12  6  4 16]]\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "How do you make a\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0 12  6  4 16  2]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "How do you make a tissue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Okm1XTb29Xdn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}